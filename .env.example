# =============================================================================
# Data-collection-pipeline — Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#
#     cp .env.example .env
#
# NEVER commit .env to version control — it contains secrets.
# .env is already listed in .gitignore.
# =============================================================================


# -----------------------------------------------------------------------------
# GitHub API (required)
# -----------------------------------------------------------------------------

# Personal Access Token for the GitHub REST API.
# Required to avoid the 60 req/hour unauthenticated rate limit.
# Authenticated limit: 5000 req/hour per token.
#
# How to generate:
#   GitHub → Settings → Developer settings →
#   Personal access tokens → Tokens (classic) → Generate new token
#   Required scope: repo (read-only is sufficient)
#
GITHUB_TOKEN=ghp_your_token_here

# Multiple tokens for rotation (recommended for datasets > 2000 projects).
# Comma-separated list — takes precedence over GITHUB_TOKEN when set.
# Each token adds 5000 req/hour:
#   2 tokens → 10,000 req/hour
#   3 tokens → 15,000 req/hour  (covers 6000+ projects per run)
#
# Tokens can belong to different GitHub accounts.
# Generate each token with the same steps as above.
#
# GITHUB_TOKENS=ghp_token1,ghp_token2,ghp_token3

# GitHub API base URL. Override only if using GitHub Enterprise.
# GITHUB_API_BASE=https://api.github.com

# Maximum number of retries on transient HTTP errors (5xx).
# GITHUB_API_MAX_RETRIES=5

# Base wait time in seconds between retries (exponential backoff).
# GITHUB_API_RETRY_BACKOFF=2.0


# -----------------------------------------------------------------------------
# Database (optional — defaults shown)
# -----------------------------------------------------------------------------

# Absolute path to the DuckDB database file.
# In Docker the default path is inside the pipeline-data volume.
# DUCKDB_PATH=/opt/airflow/data/pipeline.duckdb

# Directory for the DuckDB file and other persistent data.
# DATA_DIR=/opt/airflow/data

# Directory used as scratch space for cloning repos and running Designite.
# Can grow large depending on the number and size of projects.
# WORKSPACE_DIR=/opt/airflow/workspace


# -----------------------------------------------------------------------------
# Pipeline database (PostgreSQL) — configured automatically in docker-compose
# -----------------------------------------------------------------------------
# These are set automatically in docker-compose.yml.
# Override only if you're running pipeline-db outside of Docker.
# PIPELINE_DB_HOST=pipeline-db
# PIPELINE_DB_PORT=5432
# PIPELINE_DB_NAME=pipeline
# PIPELINE_DB_USER=pipeline
# PIPELINE_DB_PASSWORD=pipeline

# -----------------------------------------------------------------------------
# RepoQuester (required)
# -----------------------------------------------------------------------------

# Path to the RepoQuester directory inside the container.
# Mount tools/Repoquester to /opt/repoquester in docker-compose (already configured).
# REPOQUESTER_DIR=/opt/repoquester


# -----------------------------------------------------------------------------
# Designite — Java (required for Java projects)
# -----------------------------------------------------------------------------

# Absolute path to the DesigniteJava.jar file.
# Place the JAR in the tools/ folder and it will be mounted at /opt/designite/.
# DESIGNITE_JAVA_JAR=/opt/designite/DesigniteJava.jar

# Java executable. Override if java is not on PATH.
# JAVA_EXECUTABLE=java


# -----------------------------------------------------------------------------
# Designite — Python (required for Python projects)
# -----------------------------------------------------------------------------

# Absolute path to the DPy executable.
# Place DPy in the tools/ folder — it will be mounted at /opt/designite/.
# Usage inside container: /opt/designite/DPy analyze -i <input> -o <output>
# DESIGNITE_PYTHON_EXECUTABLE=/opt/designite/DPy


# -----------------------------------------------------------------------------
# DAG scheduling (optional — defaults shown)
# -----------------------------------------------------------------------------

# Cron expression or preset for the ingestion DAG.
# Presets: @daily, @hourly, @weekly
# INGESTION_SCHEDULE=@daily

# How often (in minutes) the execution DAG polls for new versions to analyse.
# EXECUTION_POLL_MINUTES=30

# Maximum number of Designite analyses running in parallel.
# Lower this if the machine runs out of memory or disk space.
# EXECUTION_MAX_ACTIVE_TASKS=4


# -----------------------------------------------------------------------------
# Airflow (required in production — can be left as-is for local dev)
# -----------------------------------------------------------------------------

# Fernet key used to encrypt sensitive data in the Airflow metadata database.
# Generate with:
#   python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here

# Secret key for the Airflow webserver session cookies.
# Generate with:
#   python -c "import secrets; print(secrets.token_hex(32))"
AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here

# JWT secret for internal authentication between Airflow components.
# Required in Airflow 3.x to avoid "Signature verification failed" errors.
# Generate with:
#   python -c "import secrets; print(secrets.token_urlsafe(64))"
AIRFLOW__API_AUTH__JWT_SECRET=your_jwt_secret_here


# -----------------------------------------------------------------------------
# Logging (optional)
# -----------------------------------------------------------------------------

# Log level for the pipeline code (not Airflow internals).
# One of: DEBUG, INFO, WARNING, ERROR
# LOG_LEVEL=INFO
