# =============================================================================
# Data-collection-pipeline — Environment Configuration
# =============================================================================
# Copy this file to .env and fill in your values:
#
#     cp .env.example .env
#
# NEVER commit .env to version control — it contains secrets.
# .env is already listed in .gitignore.
# =============================================================================


# -----------------------------------------------------------------------------
# GitHub API (required)
# -----------------------------------------------------------------------------

# Personal Access Token for the GitHub REST API.
# Required to avoid the 60 req/hour unauthenticated rate limit.
# Authenticated limit: 5000 req/hour.
#
# How to generate:
#   GitHub → Settings → Developer settings →
#   Personal access tokens → Tokens (classic) → Generate new token
#   Required scope: repo (read-only is sufficient)
#
GITHUB_TOKEN=ghp_your_token_here

# GitHub API base URL. Override only if using GitHub Enterprise.
# GITHUB_API_BASE=https://api.github.com

# Maximum number of retries on transient HTTP errors (5xx).
# GITHUB_API_MAX_RETRIES=5

# Base wait time in seconds between retries (exponential backoff).
# GITHUB_API_RETRY_BACKOFF=2.0


# -----------------------------------------------------------------------------
# Database (optional — defaults shown)
# -----------------------------------------------------------------------------

# Absolute path to the DuckDB database file.
# In Docker the default path is inside the pipeline-data volume.
# DUCKDB_PATH=/opt/airflow/data/pipeline.duckdb

# Directory for the DuckDB file and other persistent data.
# DATA_DIR=/opt/airflow/data

# Directory used as scratch space for cloning repos and running Designite.
# Can grow large depending on the number and size of projects.
# WORKSPACE_DIR=/opt/airflow/workspace


# -----------------------------------------------------------------------------
# Designite — Java (required for Java projects)
# -----------------------------------------------------------------------------

# Absolute path to the DesigniteJava.jar file.
# Place the JAR in the tools/ folder and it will be mounted at /opt/designite/.
# DESIGNITE_JAVA_JAR=/opt/designite/DesigniteJava.jar

# Java executable. Override if java is not on PATH.
# JAVA_EXECUTABLE=java


# -----------------------------------------------------------------------------
# Designite — Python (required for Python projects)
# -----------------------------------------------------------------------------

# Absolute path to the DesigniteP.py entry-point script.
# Place the script in the tools/ folder and it will be mounted at /opt/designite/.
# DESIGNITE_PYTHON_SCRIPT=/opt/designite/DesigniteP.py

# Python executable used to run DesigniteP.py.
# DESIGNITE_PYTHON_EXECUTABLE=python3


# -----------------------------------------------------------------------------
# DAG scheduling (optional — defaults shown)
# -----------------------------------------------------------------------------

# Cron expression or preset for the ingestion DAG.
# Presets: @daily, @hourly, @weekly
# INGESTION_SCHEDULE=@daily

# How often (in minutes) the execution DAG polls for new versions to analyse.
# EXECUTION_POLL_MINUTES=30

# Maximum number of Designite analyses running in parallel.
# Lower this if the machine runs out of memory or disk space.
# EXECUTION_MAX_ACTIVE_TASKS=4


# -----------------------------------------------------------------------------
# Airflow (required in production — can be left as-is for local dev)
# -----------------------------------------------------------------------------

# Fernet key used to encrypt sensitive data in the Airflow metadata database.
# Generate with:
#   python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW__CORE__FERNET_KEY=your_fernet_key_here

# Secret key for the Airflow webserver session cookies.
# Generate with:
#   python -c "import secrets; print(secrets.token_hex(32))"
AIRFLOW__WEBSERVER__SECRET_KEY=your_secret_key_here


# -----------------------------------------------------------------------------
# Logging (optional)
# -----------------------------------------------------------------------------

# Log level for the pipeline code (not Airflow internals).
# One of: DEBUG, INFO, WARNING, ERROR
# LOG_LEVEL=INFO
