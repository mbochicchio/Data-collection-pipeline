# ---------------------------------------------------------------------------
# Data-collection-pipeline — Docker Compose (local dev)
# Compatible with Apache Airflow 3.1.7
#
# Usage:
#   cp .env.example .env          # fill in GITHUB_TOKEN etc.
#   ./init.sh                     # first-time setup
#   ./start.sh                    # start pipeline (Ctrl+C to stop + backup)
#   open http://localhost:8080    # Airflow UI  (admin / admin)
# ---------------------------------------------------------------------------

x-airflow-common: &airflow-common
  build:
    context: .
    dockerfile: Dockerfile
  image: github-pipeline:latest
  env_file: .env
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    AIRFLOW__CORE__DAGS_FOLDER: /opt/airflow/pipeline/dags
    AIRFLOW__CORE__AUTH_MANAGER: airflow.providers.fab.auth_manager.fab_auth_manager.FabAuthManager
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://airflow-apiserver:8080/execution/"
    # Workaround for Airflow 3.1.7 bug — JWT secret must be set explicitly
    AIRFLOW__API_AUTH__JWT_SECRET: ${AIRFLOW__API_AUTH__JWT_SECRET}
    AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
    AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
    AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session"
    # Pipeline database connection
    PIPELINE_DB_HOST: pipeline-db
    PIPELINE_DB_PORT: "5432"
    PIPELINE_DB_NAME: pipeline
    PIPELINE_DB_USER: pipeline
    PIPELINE_DB_PASSWORD: pipeline
    WORKSPACE_DIR: /opt/airflow/workspace
    PYTHONPATH: /opt/airflow/pipeline
  volumes:
    - ./dags:/opt/airflow/pipeline/dags
    - ./common:/opt/airflow/pipeline/common
    - ./config:/opt/airflow/pipeline/config
    - ./plugins:/opt/airflow/pipeline/plugins
    - ./scripts:/opt/airflow/pipeline/scripts
    - ./logs:/opt/airflow/logs
    - ./data/repo_urls.txt:/opt/airflow/pipeline/data/repo_urls.txt:ro
    - pipeline-workspace:/opt/airflow/workspace
    - ./tools/Designite:/opt/designite
    - ./tools/Repoquester:/opt/repoquester
  depends_on:
    postgres:
      condition: service_healthy
    pipeline-db:
      condition: service_healthy
  restart: unless-stopped

services:

  # ---------------------------------------------------------------------------
  # PostgreSQL — Airflow metadata database
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # PostgreSQL — Pipeline data database
  # ---------------------------------------------------------------------------
  pipeline-db:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: pipeline
      POSTGRES_PASSWORD: pipeline
      POSTGRES_DB: pipeline
    ports:
      - "5434:5432"
    volumes:
      - pipeline-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "pipeline"]
      interval: 10s
      retries: 5
      start_period: 5s
    restart: unless-stopped

  # ---------------------------------------------------------------------------
  # Airflow initialisation — runs once then exits
  # ---------------------------------------------------------------------------
  airflow-init:
    <<: *airflow-common
    profiles: ["init"]
    command: bash -c "airflow db migrate && airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com && python /opt/airflow/pipeline/scripts/init_db.py --seed /opt/airflow/pipeline/data/repo_urls.txt"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Airflow API server (replaces "webserver" in Airflow 3.x)
  # ---------------------------------------------------------------------------
  airflow-apiserver:
    <<: *airflow-common
    command: api-server
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Airflow scheduler
  # ---------------------------------------------------------------------------
  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "SchedulerJob", "--local"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Airflow DAG processor (required in Airflow 3.x)
  # ---------------------------------------------------------------------------
  airflow-dag-processor:
    <<: *airflow-common
    command: dag-processor
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "DagProcessorJob", "--local"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Airflow triggerer (required in Airflow 3.x)
  # ---------------------------------------------------------------------------
  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD", "airflow", "jobs", "check", "--job-type", "TriggererJob", "--local"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

volumes:
  postgres-data:
  pipeline-db-data:
  pipeline-workspace: